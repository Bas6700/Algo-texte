{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_df = pd.read_csv(\"allocine_genres_train.csv\", sep=\",\")\n",
    "# Liste des classes\n",
    "class_names = sorted(film_df.genre.unique())\n",
    "# On associe à chaque classe un identifiant unique\n",
    "class_index = {class_names[i]:i for i in range(len(class_names))}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des différentes fonctions utilisé plus tard:\n",
    "get_vectorizer permet la vectorisation du texte et\n",
    "load_embenddings fait ce que son nom indique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(documents, max_voc_size=8000, max_seq_length=50, batch_size=128):\n",
    "  vectorizer = TextVectorization(max_tokens=max_voc_size, \n",
    "                                 output_sequence_length=max_seq_length)\n",
    "  # Création du jeu de données à partir de X_train et constitution de lots de 128 instances\n",
    "  text_ds = tf.data.Dataset.from_tensor_slices(documents).batch(batch_size)\n",
    "  # Création du vocabulaire à partir des données d'entrée\n",
    "  vectorizer.adapt(text_ds)\n",
    "  return vectorizer\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_file):\n",
    "  embeddings_index = {}\n",
    "  with open(embeddings_file, 'r', encoding='utf8') as f:\n",
    "      try:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "      except:\n",
    "        ; #un seul mot qui n'est pas lisible en utf8 donc il est ignoré\n",
    "  print(f'{len(embeddings_index)} vecteurs de mots ont été lus')\n",
    "  return embeddings_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le modèle utilisé (model.txt) pour le plongement de mots, il faut prendre sur le site http://vectors.nlpl.eu/repository/# le modèle ayant l'id numéro 51. (le code est adapté à ce modèle. Pour l'autre site internet mit dans le rapport en bibliographie, il faut utiliser l'autre fichier projet_BILSTM_FP.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise uniquement le synopsis et le titre comme données d'entrée\n",
    "X_train = film_df.titre + ' ' + film_df.synopsis\n",
    "# Les noms des classes sont remplacées par leur identifiant (un entier positif)\n",
    "y_train = film_df.genre.map(class_index)\n",
    "keras_vectorizer = get_vectorizer(X_train)\n",
    "voc = keras_vectorizer.get_vocabulary()\n",
    "# on associe un identifiant unique à chaque item du vocabulaire\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "# Chargement des plongements du fichier model.txt\n",
    "m_embeddings = load_embeddings('model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(vocabulary, embeddings_index, embedding_dim = 100):\n",
    "  num_tokens = len(vocabulary)\n",
    "  hits = 0\n",
    "  misses = 0\n",
    "  # Préparation de la matrice\n",
    "  # Les mots qui ne se trouvent pas dans les plongements pré-entraînés seront \n",
    "  # représentés par des vecteurs dont toutes les composantes sont égales à 0,\n",
    "  # y compris la représentation utilisée pour compléter les documents courts et\n",
    "  # celle utilisée pour les mots inconnus [UNK]\n",
    "  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "  for word, i in word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is None or embedding_vector.shape == (0,) :\n",
    "        misses += 1     \n",
    "      else:\n",
    "        embedding_matrix[i] = embedding_vector[:100] #certain on 101 au lieu de 100\n",
    "        hits += 1\n",
    "  print(f'{hits} mots ont été trouvés dans les plongements pré-entraînés')\n",
    "  print(f'{misses} sont absents')\n",
    "  return embedding_matrix\n",
    "# Construction de la matrice de plongements à partir du vocabulaire\n",
    "m_embedding_matrix = get_embedding_matrix(voc, m_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biLSTM_model(voc_size, embedding_matrix, embedding_dim=100):\n",
    "  # Création du modèle\n",
    "  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "  embedding_layer = Embedding(voc_size, embedding_dim, trainable=True,\n",
    "      embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "  )\n",
    "  \n",
    "  embedded_sequences = embedding_layer(int_sequences_input)\n",
    "  x = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))(embedded_sequences)\n",
    "  preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "  model = keras.Model(int_sequences_input, preds)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de l'architecture du modèle\n",
    "biLSTM_model = get_biLSTM_model(len(voc), m_embedding_matrix)\n",
    "biLSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'entraînement d'un modèle\n",
    "def train_model(X, y, model_function, vectorizer,\n",
    "                voc_size, embedding_matrix, embedding_dim=100, batch_size=128):\n",
    "  # Listes utilisées pour sauvegarder les résultats obtenus à chaque pli\n",
    "  acc_per_fold = []\n",
    "  loss_per_fold = []\n",
    "  histories = []\n",
    "  folds = 5\n",
    "  stratkfold = model_selection.StratifiedKFold(n_splits=folds, shuffle=True, \n",
    "                                              random_state=12)\n",
    "  fold_no = 1\n",
    "  for train, test in stratkfold.split(X, y):\n",
    "    m_function = globals()[model_function]\n",
    "    model = m_function(voc_size, \n",
    "                       embedding_matrix, \n",
    "                       embedding_dim)\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Entraînement pour le pli {fold_no} ...')\n",
    "    fold_x_train = vectorizer(X.iloc[train].to_numpy()).numpy()\n",
    "    fold_x_val = vectorizer(X.iloc[test].to_numpy()).numpy()\n",
    "    fold_y_train = y.iloc[train].to_numpy()\n",
    "    fold_y_val = y.iloc[test].to_numpy()\n",
    "\n",
    "    # Compilation du modèle : permet de préciser la fonction de perte et l'optimiseur\n",
    "    # loss=sparse_categorical_crossentropy : entropie croisée, dans le cas où les \n",
    "    #  classes cibles sont indiquées sous forme d'entiers. Il s'agira de minimiser\n",
    "    #  la perte pendant l'apprentissage\n",
    "    # optimizer=rmsprop : l'optimiseur détermine la manière doit les poids seront\n",
    "    #  mis à jour pendant l'apprentissage\n",
    "    model.compile(\n",
    "      loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    "    )\n",
    "    # Entraînement\n",
    "    history = model.fit(fold_x_train, fold_y_train, batch_size=batch_size, \n",
    "                        epochs=10, validation_data=(fold_x_val, fold_y_val))\n",
    "    histories.append(history)\n",
    "    # Evaluation sur les données de validation\n",
    "    scores = model.evaluate(fold_x_val, fold_y_val, verbose=0)\n",
    "    print(f'Scores pour le pli {fold_no}: {model.metrics_names[0]} = {scores[0]:.2f};',\n",
    "          f'{model.metrics_names[1]} = {scores[1]*100:.2f}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "  # Affichage des scores moyens par pli\n",
    "  print('---------------------------------------------------------------------')\n",
    "  print('Scores par pli')\n",
    "  for i in range(0, len(acc_per_fold)):\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(f'> Pli {i+1} - Loss: {loss_per_fold[i]:.2f}',\n",
    "          f'- Accuracy: {acc_per_fold[i]:.2f}%')\n",
    "  print('---------------------------------------------------------------------')\n",
    "  print('Scores moyens pour tous les plis :')\n",
    "  print(f'> Accuracy: {np.mean(acc_per_fold):.2f}',\n",
    "        f'(+- {np.std(acc_per_fold):.2f})')\n",
    "  print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n",
    "  print('---------------------------------------------------------------------')\n",
    "  return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle et récupération des résultats\n",
    "biLSTM_histories = train_model(X_train, y_train, 'get_biLSTM_model',\n",
    "                            keras_vectorizer, len(voc), m_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(histories):\n",
    "  accuracy_data = []\n",
    "  loss_data = []\n",
    "  for i, h in enumerate(histories):\n",
    "    acc = h.history['acc']\n",
    "    val_acc = h.history['val_acc']\n",
    "    loss = h.history['loss']\n",
    "    val_loss = h.history['val_loss']\n",
    "    for j in range(len(acc)):\n",
    "      accuracy_data.append([i+1, j+1, acc[j], 'Entraînement'])\n",
    "      accuracy_data.append([i+1, j+1, val_acc[j], 'Validation'])\n",
    "      loss_data.append([i+1, j+1, loss[j], 'Entraînement'])\n",
    "      loss_data.append([i+1, j+1, val_loss[j], 'Validation'])\n",
    "\n",
    "  acc_df = pd.DataFrame(accuracy_data, \n",
    "                        columns=['Pli', 'Epoch', 'Accuracy', 'Données'])\n",
    "  sns.relplot(data=acc_df, x='Epoch', y='Accuracy', hue='Pli', style='Données',\n",
    "              kind='line')\n",
    "    \n",
    "  loss_df = pd.DataFrame(loss_data, columns=['Pli', 'Epoch', 'Perte', 'Données'])\n",
    "  sns.relplot(data=loss_df, x='Epoch', y='Perte', hue='Pli', style='Données',\n",
    "              kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(biLSTM_histories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
