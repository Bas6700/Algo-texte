{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports nécessaires\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppression de l'affichage des messages d'avertissement\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string\n",
    "import time\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "from keras import layers\n",
    "from sparknlp.base import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import set_config\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding\n",
    "from spacy.lang.fr import French \n",
    "\n",
    "\n",
    "# Pour éviter l'affichage tronqué des descriptions\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "# Pour la visualisation des pipelines sklearn\n",
    "set_config(display='diagram') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des différentes fonctions utilisé plus tard:\n",
    "get_vectorizer permet la vectorisation du texte et\n",
    "load_embenddings fait ce que son nom indique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(documents, max_voc_size=8000, max_seq_length=50, batch_size=128):\n",
    "  vectorizer = TextVectorization(max_tokens=max_voc_size, \n",
    "                                 output_sequence_length=max_seq_length)\n",
    "  # Création du jeu de données à partir de X_train et constitution de lots de 128 instances\n",
    "  text_ds = tf.data.Dataset.from_tensor_slices(documents).batch(batch_size)\n",
    "  # Création du vocabulaire à partir des données d'entrée\n",
    "  vectorizer.adapt(text_ds)\n",
    "  return vectorizer\n",
    "\n",
    "def load_embeddings(embeddings_file):\n",
    "  embeddings_index = {}\n",
    "  with open(embeddings_file, 'r', encoding='utf8') as f:\n",
    "      try:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "      except:\n",
    "        ; #un seul mot qui n'est pas lisible en utf8 donc il est ignoré\n",
    "  print(f'{len(embeddings_index)} vecteurs de mots ont été lus')\n",
    "  return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_df = pd.read_csv(\"allocine_genres_train.csv\", sep=\",\")\n",
    "# Liste des classes\n",
    "class_names = sorted(film_df.genre.unique())\n",
    "# On associe à chaque classe un identifiant unique\n",
    "class_index = {class_names[i]:i for i in range(len(class_names))}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le modèle (model.txt) utilisé pour le plongement de mots, il faut prendre sur le site http://vectors.nlpl.eu/repository/# le modèle ayant l'id numéro 51. (le code est adapté à ce modèle. Pour l'autre site internet mit dans le rapport en bibliographie, il faut utiliser l'autre fichier projet_CNN_FP.ipynb)\n",
    "Dans le code qui suit, la matrice d'embeddings n'est pas utilisé car elle donne de moins bon résultats si elle est utilisé.\n",
    "Néamoins le code est en commentaire s'il doit être utilisé. Il faut décommenter à deux endroits qui sont indiqués dans le code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise uniquement le synopsis et le titre comme données d'entrée\n",
    "X_train = film_df.titre + ' ' + film_df.synopsis\n",
    "# Les noms des classes sont remplacées par leur identifiant (un entier positif)\n",
    "y_train = film_df.genre.map(class_index)\n",
    "keras_vectorizer = get_vectorizer(X_train)\n",
    "voc = keras_vectorizer.get_vocabulary()\n",
    "# on associe un identifiant unique à chaque item du vocabulaire\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "# Chargement des plongements du fichier model.txt\n",
    "m_embeddings = load_embeddings('model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(vocabulary, embeddings_index, embedding_dim = 100):\n",
    "  num_tokens = len(vocabulary)\n",
    "  hits = 0\n",
    "  misses = 0\n",
    "  # Préparation de la matrice\n",
    "  # Les mots qui ne se trouvent pas dans les plongements pré-entraînés seront \n",
    "  # représentés par des vecteurs dont toutes les composantes sont égales à 0,\n",
    "  # y compris la représentation utilisée pour compléter les documents courts et\n",
    "  # celle utilisée pour les mots inconnus [UNK]\n",
    "  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "  for word, i in word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is None or embedding_vector.shape == (0,) :\n",
    "        misses += 1     \n",
    "      else:\n",
    "        embedding_matrix[i] = embedding_vector[:100] #certain on 101 au lieu de 100\n",
    "        hits += 1\n",
    "  print(f'{hits} mots ont été trouvés dans les plongements pré-entraînés')\n",
    "  print(f'{misses} sont absents')\n",
    "  return embedding_matrix\n",
    "# Construction de la matrice de plongements à partir du vocabulaire\n",
    "m_embedding_matrix = get_embedding_matrix(voc, m_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pour utiliser la matrice d'embedding, il faut décommenter le code ici\n",
    "def get_CNN_model(voc_size, embedding_matrix, embedding_dim=100):\n",
    "  # Création du modèle\n",
    "  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "  embedding_layer = Embedding(voc_size, embedding_dim, trainable=True,\n",
    "      #embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
    "  )\n",
    "  \n",
    "  embedded_sequences = embedding_layer(int_sequences_input)\n",
    "  x = layers.Conv1D(64, 5, activation=\"relu\")(embedded_sequences)\n",
    "  x = layers.MaxPooling1D(5)(x)\n",
    "  x = layers.Conv1D(64, 5, activation=\"relu\")(x)\n",
    "  x = layers.GlobalMaxPooling1D()(x)\n",
    "  x = layers.Dense(64, activation=\"relu\")(x)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "  model = keras.Model(int_sequences_input, preds)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'entraînement d'un modèle\n",
    "def train_model(X, y, model_function, vectorizer,\n",
    "                voc_size, embedding_matrix, embedding_dim=100, batch_size=128):\n",
    "  # Listes utilisées pour sauvegarder les résultats obtenus à chaque pli\n",
    "  acc_per_fold = []\n",
    "  loss_per_fold = []\n",
    "  histories = []\n",
    "  folds = 5\n",
    "  stratkfold = model_selection.StratifiedKFold(n_splits=folds, shuffle=True, \n",
    "                                              random_state=12)\n",
    "  fold_no = 1\n",
    "  # Pour utiliser la matrice d'embedding, il faut aussi décommenter le code ici\n",
    "  for train, test in stratkfold.split(X, y):\n",
    "    m_function = globals()[model_function]\n",
    "    model = m_function(voc_size, \n",
    "                      #embedding_matrix, \n",
    "                       embedding_dim)\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Entraînement pour le pli {fold_no} ...')\n",
    "    fold_x_train = vectorizer(X.iloc[train].to_numpy()).numpy()\n",
    "    fold_x_val = vectorizer(X.iloc[test].to_numpy()).numpy()\n",
    "    fold_y_train = y.iloc[train].to_numpy()\n",
    "    fold_y_val = y.iloc[test].to_numpy()\n",
    "\n",
    "    # Compilation du modèle : permet de préciser la fonction de perte et l'optimiseur\n",
    "    # loss=sparse_categorical_crossentropy : entropie croisée, dans le cas où les \n",
    "    #  classes cibles sont indiquées sous forme d'entiers. Il s'agira de minimiser\n",
    "    #  la perte pendant l'apprentissage\n",
    "    # optimizer=rmsprop : l'optimiseur détermine la manière doit les poids seront\n",
    "    #  mis à jour pendant l'apprentissage\n",
    "    model.compile(\n",
    "      loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    "    )\n",
    "    # Entraînement sur 10 époque\n",
    "    history = model.fit(fold_x_train, fold_y_train, batch_size=batch_size, \n",
    "                        epochs=10, validation_data=(fold_x_val, fold_y_val))\n",
    "    histories.append(history)\n",
    "    # Evaluation sur les données de validation\n",
    "    scores = model.evaluate(fold_x_val, fold_y_val, verbose=0)\n",
    "    print(f'Scores pour le pli {fold_no}: {model.metrics_names[0]} = {scores[0]:.2f};',\n",
    "          f'{model.metrics_names[1]} = {scores[1]*100:.2f}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "  # Affichage des scores moyens par pli\n",
    "  print('---------------------------------------------------------------------')\n",
    "  print('Scores par pli')\n",
    "  for i in range(0, len(acc_per_fold)):\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(f'> Pli {i+1} - Loss: {loss_per_fold[i]:.2f}',\n",
    "          f'- Accuracy: {acc_per_fold[i]:.2f}%')\n",
    "  print('---------------------------------------------------------------------')\n",
    "  print('Scores moyens pour tous les plis :')\n",
    "  print(f'> Accuracy: {np.mean(acc_per_fold):.2f}',\n",
    "        f'(+- {np.std(acc_per_fold):.2f})')\n",
    "  print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n",
    "  print('---------------------------------------------------------------------')\n",
    "  return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(histories):\n",
    "  accuracy_data = []\n",
    "  loss_data = []\n",
    "  for i, h in enumerate(histories):\n",
    "    acc = h.history['acc']\n",
    "    val_acc = h.history['val_acc']\n",
    "    loss = h.history['loss']\n",
    "    val_loss = h.history['val_loss']\n",
    "    for j in range(len(acc)):\n",
    "      accuracy_data.append([i+1, j+1, acc[j], 'Entraînement'])\n",
    "      accuracy_data.append([i+1, j+1, val_acc[j], 'Validation'])\n",
    "      loss_data.append([i+1, j+1, loss[j], 'Entraînement'])\n",
    "      loss_data.append([i+1, j+1, val_loss[j], 'Validation'])\n",
    "\n",
    "  acc_df = pd.DataFrame(accuracy_data, \n",
    "                        columns=['Pli', 'Epoch', 'Accuracy', 'Données'])\n",
    "  sns.relplot(data=acc_df, x='Epoch', y='Accuracy', hue='Pli', style='Données',\n",
    "              kind='line')\n",
    "    \n",
    "  loss_df = pd.DataFrame(loss_data, columns=['Pli', 'Epoch', 'Perte', 'Données'])\n",
    "  sns.relplot(data=loss_df, x='Epoch', y='Perte', hue='Pli', style='Données',\n",
    "              kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle et récupération des résultats\n",
    "CNN_histories = train_model(X_train, y_train, 'get_CNN_model',\n",
    "                            keras_vectorizer, len(voc), m_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(CNN_histories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
